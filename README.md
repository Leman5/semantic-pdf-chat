# ğŸ§  LangChain PDF QA System using Streamlit

![license](https://img.shields.io/github/license/Leman5/semantic-pdf-chat)
![python](https://img.shields.io/badge/Python-3.10%2B-blue)
![streamlit](https://img.shields.io/badge/Streamlit-ğŸŒŸ-brightgreen)

A  **Questionâ€‘Answering app** that lets you upload a PDF and ask naturalâ€‘language questions about its contents. The app semantically chunks the document, embeds those chunks, retrieves only the 3 most relevant ones, compresses them with an LLM, and finally feeds the concise context to GPTâ€‘4 to craft an answer.

### Preview
![App Screenshot](./Screenshot%20from%202025-06-04%2010-18-09.png)

---

## âš¡ QuickÂ Start

```bash
# 1.Â Clone & install
$ git clone https://github.com/Leman5/semantic-pdf-chat.git
$ cd semantic-pdf-chat
$ python -m venv venv && source venv/bin/activate
$ pip install -r requirements.txt

# 2.Â Add your keys in .env file

# 3.Â Run ğŸƒ
$ streamlit run app.py
```

---

## ğŸ” DesignÂ Decisions & KeyÂ Parameters

| Layer                           | Library                                                                                                                                                                                                                                             | Parameters                                       | Why this choice?                                                                                                                                                             |
| ------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Chunking**                    | `SemanticChunker` (LangChainÂ Experimental)                                                                                                                                                                                                          | `breakpoint_threshold_type="standard_deviation"` |                                                                                                                                                                              |
| `breakpoint_threshold_amount=3` | Semantic breaks respect topic shifts and sentence boundaries, preserving coherence better than naÃ¯ve fixedâ€‘length splits. The threshold of **3Â Ïƒ** filters only strong semantic jumps, giving \~500â€‘1â€¯000Â token chunksâ€”ideal for OpenAI embeddings. |                                                  |                                                                                                                                                                              |
| **Vector Store**                | **Chroma** (persistent)                                                                                                                                                                                                                             | `collection_name="sample"`                       | Chroma offers millisecondâ€‘level similarity search, local persistence, and effortless versioningâ€”perfect for desktop or smallâ€‘team deployments.                               |
| **Retrieval**                   | `.as_retriever(k=3)`                                                                                                                                                                                                                                | `kÂ =Â 3` (topÂ 3 docs)                             | Empirically, 1â€“3 highly relevant chunks outperform larger sets by avoiding distraction & keeping prompts <Â 8â€¯K tokens. **3** hits the sweet spot between recall and latency. |
| **Compression**                 | `LLMChainExtractor` (GPTâ€‘3.5â€‘turbo,Â T=0.0)                                                                                                                                                                                                          | N/A                                              | Uses a fast, deterministic model to prune fluff and keep only sentences that answer the queryâ€”reducing cost and context size by \~60â€¯%.                                      |
| **AnswerÂ LLM**                  | `ChatOpenAI` (GPTâ€‘4,Â T=0.2)                                                                                                                                                                                                                         | `temperatureÂ =Â 0.2`                              | Slight randomness encourages natural language while remaining factual.                                                                                                       |

---

## ğŸ› ï¸ TechÂ Stack

* **PythonÂ 3.10+**
* **Streamlit** UI
* **LangChain** orchestration

  * `SemanticChunker`
  * `ContextualCompressionRetriever`
* **OpenAI** embeddings & chat models
* **Chroma** vector DB (local)
* **BeautifulSoup4** (optional URL ingestion)
* **pythonâ€‘dotenv** for secrets

---

## ğŸ—ºï¸ DataÂ Flow

```
ğŸ“„ PDF â†’ PyPDFLoader
        â†“
   SemanticChunker  â”€â”€â”€â”€â”€â–¶  ChromaÂ (vectorÂ store)
        â†“                      â–²            
        â–¼                      â”‚            
ContextualCompressionRetriever â”‚            
        â†“                      â”‚            
  topâ€‘3Â compressedÂ chunks  â”€â”€â”€â”€â”€â”˜            
        â†“                                   
     PromptÂ Template (contextÂ +Â question)
        â†“
      GPTâ€‘4 â†’ âœ¨Â Answer
```

---

## ğŸŒŸ Why is this approach effective?

1. **CoherentÂ ChunksÂ â‡¢ BetterÂ Embeddings** â€“ Semantic boundaries yield embeddings that truly represent the idea in each passage.
2. **SmallÂ kÂ =Â FastÂ &Â Focused** â€“ Retrieving only 3 chunks keeps prompts short, lowers latency & cost, and reduces hallucination by limiting noise.
3. **CompressionÂ BeforeÂ Generation** â€“ Stripping excess sentences lets GPT spend its context window on signal, not filler.
4. **SeparationÂ ofÂ Concerns** â€“ Different models do what they do best: GPTâ€‘3.5â€‘turbo for cheap extraction, GPTâ€‘4 for highâ€‘quality answers.

---

## ğŸ“‘ EnvironmentÂ Variables

| Name                           | Purpose                            |
| ------------------------------ | ---------------------------------- |
| `OPENAI_API_KEY`               | Your OpenAI key                    |

Create a `.env` file:

```env
OPENAI_API_KEY=skâ€‘...
```

---

